{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *   # bring everything in the folder models\n",
    "from models.project_vgg import *\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "\n",
    "model_name = \"project_part2_VGG16_quant\"\n",
    "#model = cifar10()\n",
    "# model = VGG19()\n",
    "\n",
    "model = VGG16_quant_project() # VGG16()\n",
    "\n",
    "# model = model.cuda() # use din the cell below\n",
    "        \n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()   ## at the begining of each epoch, this should be reset\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()  # measure current time\n",
    "    \n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)  # data loading time\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end) # time spent to process one batch\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,5)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk) # 5\n",
    "    batch_size = target.size(0) # 128\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True) # topk(k, dim=None, largest=True, sorted=True)\n",
    "                                    # will output (max value, its index)\n",
    "    pred = pred.t()               # transpose\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))   # \"-1\": calculate automatically\n",
    "\n",
    "    res = []\n",
    "    for k in topk: # 1, 5\n",
    "        # correct_k = correct[:k].view(-1).float().sum(0)  # view(-1): make a flattened 1D tensor\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)  # view(-1): make a flattened 1D tensor\n",
    "        \n",
    "        res.append(correct_k.mul_(100.0 / batch_size))   # correct: size of [maxk, batch_size]\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n    ## n is impact factor\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'vgg16_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "small-favorite",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    from IPython.display import clear_output\n",
    "    \n",
    "    lr = 1e-3\n",
    "    # lr = 0.1\n",
    "    # lr = 0.05\n",
    "    \n",
    "    # weight_decay = 1e-4\n",
    "    weight_decay = 0 # 5e-4\n",
    "    epochs = 200 # orig: 30\n",
    "    best_prec = 0\n",
    "    \n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "    # weight decay: for regularization to prevent overfitting\n",
    "    \n",
    "    if not os.path.exists('result'):\n",
    "        os.makedirs('result')\n",
    "        \n",
    "    fdir = 'result/'+str(model_name)\n",
    "    \n",
    "    if not os.path.exists(fdir):\n",
    "        os.makedirs(fdir)\n",
    "    \n",
    "    decay = []\n",
    "    no_decay = []\n",
    "    \n",
    "    # keep weight decay nonzero for the full precision layers:\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "    \n",
    "        # First and last layers of VGG16 (full precision)\n",
    "        if \"features.0\" in name or \"classifier\" in name:\n",
    "            decay.append(param) #use weight decay\n",
    "        else:\n",
    "            no_decay.append(param) # quantized layers get no WD\n",
    "    \n",
    "    optimizer = torch.optim.SGD(\n",
    "        [\n",
    "            {\"params\": decay, \"weight_decay\": 5e-4},\n",
    "            {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "        ],\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        # adjust_learning_rate(optimizer, epoch)\n",
    "        \n",
    "        # clear_output(wait=True)\n",
    "        # print('best acc: {:1f}'.format(best_prec))\n",
    "    \n",
    "        train(trainloader, model, criterion, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "        # evaluate on test set\n",
    "        \n",
    "        \n",
    "        print(\"Validation starts\")\n",
    "        prec = validate(testloader, model, criterion)\n",
    "    \n",
    "        # remember best precision and save checkpoint\n",
    "        is_best = prec > best_prec\n",
    "        best_prec = max(prec,best_prec)\n",
    "        print('best acc: {:1f}'.format(best_prec))\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec': best_prec,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, is_best, fdir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hydraulic-passport",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/79]\tTime 0.507 (0.507)\tLoss 0.3292 (0.3292)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.050% \n"
     ]
    }
   ],
   "source": [
    "# model_name = 'VGG16'\n",
    "\"\"\"\"\n",
    "LOADING MODEL AND TESTING PART\n",
    "\"\"\"\n",
    "fdir = 'result/'+str(model_name)+'/2bit_training_best_above85.tar'\n",
    "\n",
    "checkpoint = torch.load(fdir)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "prec = validate(testloader, model, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 28 input: torch.Size([128, 16, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "\n",
    "model.features[27].register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "save_input28 = SaveOutput()\n",
    "model.features[28].register_forward_pre_hook(save_input28)\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "\n",
    "\n",
    "# Input to layer 28:\n",
    "layer28_input = save_input28.outputs[0][0]   # (tuple → [0])\n",
    "print(\"Layer 28 input:\", layer28_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-pitch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5040., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weight_q = model.features[27].weight_q\n",
    "w_alpha = model.features[27].weight_quant.wgt_alpha\n",
    "w_bit = 4\n",
    "\n",
    "weight_int = weight_q / (w_alpha / (2**(w_bit-1)-1))\n",
    "#print(weight_int)\n",
    "print(weight_int.abs().sum()) #should be integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blond-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 16, 4, 4])\n",
      "tensor(9707., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "act = save_output.outputs[0][0]\n",
    "print(act.size())\n",
    "act_alpha  = model.features[27].act_alpha\n",
    "act_bit = 2\n",
    "act_quant_fn = act_quantization(act_bit)\n",
    "\n",
    "act_q = act_quant_fn(act, act_alpha)\n",
    "\n",
    "act_int = torch.round(act_q / (act_alpha / (2**act_bit-1)))\n",
    "#print(act_int)\n",
    "print(act_int.abs().sum()) #should be an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0239a29-e901-4492-bb09-1b6ee6993cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act alpha:  5.192733287811279\n",
      "weight alpha: 2.63910174369812\n",
      "torch.Size([1, 16, 2, 2])\n",
      "torch.Size([1, 16, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "a_int = act_int[0].unsqueeze(0) # [1, 16, 4, 4]\n",
    "device = a_int.device\n",
    "\n",
    "# must use the regulsar conv layer - if I use the custom one that I built for quantization part, it will quantize 2 times and cause an error\n",
    "conv_int = nn.Conv2d(\n",
    "    in_channels=16,\n",
    "    out_channels=16,\n",
    "    kernel_size=3,\n",
    "    padding=0,\n",
    "    bias=False\n",
    ").to(device)\n",
    "\n",
    "conv_int.weight = torch.nn.Parameter(weight_int.clone())\n",
    "conv_int.bias = None\n",
    "\n",
    "output_int = conv_int(a_int)        # [1, 16, 4, 4]\n",
    "\n",
    "scale_act = act_alpha / (2**act_bit - 1)\n",
    "scale_w   = w_alpha   / (2**(w_bit-1) - 1)\n",
    "\n",
    "output_recovered = output_int * scale_act * scale_w\n",
    "\n",
    "# print(\"Recovered output (float):\", output_recovered[0])\n",
    "print(\"act alpha: \", act_alpha.item())\n",
    "print(\"weight alpha:\", w_alpha.item())\n",
    "print(output_int.size())\n",
    "print(output_recovered.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d72cd4-76f4-49ae-b7f9-ca3c8ce2a7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.762695789337158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "496.8125305175781"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = abs( output_int - output_recovered)\n",
    "print(diff.mean().item())\n",
    "diff.abs().sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bda31720-a05c-4daf-a970-765400db2429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act quantization error sum: 7053.25927734375\n",
      "act quantization MSE: 0.12529322504997253\n",
      "act quantization RMSE: 0.3539678156375885\n",
      "act int to float reconstruction error: 7053.25927734375\n",
      "weight quantization L1: 1806.7098388671875\n",
      "weight quantization MSE: 0.9202349185943604\n",
      "weight quantization RMSE: 0.9592887759208679\n",
      "weight int reconstruction error: 1806.7098388671875\n",
      "output L1 quant error: 404227.96875\n",
      "output RMSE quant error: 15.826031684875488\n",
      "output max error: 80.21974182128906\n"
     ]
    }
   ],
   "source": [
    "def rmse(x, y):\n",
    "    return torch.sqrt(((x - y) ** 2).mean())\n",
    "\n",
    "act_rmse = rmse(act, act_q)\n",
    "act_error = act - act_q\n",
    "print(\"act quantization error sum:\", act_error.abs().sum().item())\n",
    "print(\"act quantization MSE:\", (act_error**2).mean().item())\n",
    "print(\"act quantization RMSE:\",act_rmse.item())\n",
    "\n",
    "act_reconstructed = act_int * (act_alpha / (2**act_bit - 1))\n",
    "act_int_error = act - act_reconstructed\n",
    "\n",
    "print(\"act int to float reconstruction error:\", act_int_error.abs().sum().item())\n",
    "\n",
    "weight_float = model.features[27].weight\n",
    "weight_q     = model.features[27].weight_q\n",
    "weight_rmse = rmse(weight_float, weight_q)\n",
    "\n",
    "\n",
    "w_error = weight_float - weight_q\n",
    "print(\"weight quantization L1:\", w_error.abs().sum().item())\n",
    "print(\"weight quantization MSE:\", (w_error**2).mean().item())\n",
    "print(\"weight quantization RMSE:\", weight_rmse.item())\n",
    "\n",
    "w_reconstructed = weight_int * (w_alpha / (2**(w_bit-1)-1))\n",
    "w_int_error = weight_float - w_reconstructed\n",
    "print(\"weight int reconstruction error:\", w_int_error.abs().sum().item())\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_float = F.conv2d(act, weight_float, None, 1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_quant = F.conv2d(act_q, weight_q, None, 1, 1)\n",
    "\n",
    "out_error = out_float - out_quant\n",
    "# out_rmse = rmse(out_float, out_quant)\n",
    "print(\"output L1 quant error:\", out_error.abs().sum().item())\n",
    "print(\"output RMSE quant error:\", torch.sqrt((out_error**2).mean()).item())\n",
    "print(\"output max error:\", out_error.abs().max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5df41b41-a7af-4307-b14e-48743fa7daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2, 2])\n",
      "tensor(1430., device='cuda:0', grad_fn=<SumBackward0>)\n",
      "torch.Size([8, 16])\n",
      "2\n",
      "2\n",
      "torch.Size([2, 2, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "########### TILED CALCULATION HERE #################\n",
    "# taken from the hw solution:\n",
    "import math\n",
    "\n",
    "# act_int.size = torch.Size([128, 64, 32, 32])  <- batch_size, input_ch, ni, nj\n",
    "a_int = act_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [64, 32, 32]\n",
    "\n",
    "# conv_int.weight.size() = torch.Size([64, 64, 3, 3])  <- output_ch, input_ch, ki, kj\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([64, 64, 9])\n",
    "                      \n",
    "padding = 0\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "######## Inputs ########\n",
    "nig = range(a_int.size(1))  ## ni group [0,1,...31]\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "nijg = range(a_int.size(1)*a_int.size(2))\n",
    "\n",
    "######## Weights and related stuff ########\n",
    "kijg = range(w_int.size(2)) # [0, .. 8]\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size = 3\n",
    "kig = range(int(math.sqrt(len(kijg)))) ## = 3\n",
    "kjg = range(int(math.sqrt(len(kijg)))) ## = 3\n",
    "\n",
    "######## Channels ########\n",
    "icg = range(int(w_int.size(1)))  ## input channel [0,...63]\n",
    "ocg = range(int(w_int.size(0)))  ## output channel [0,...63]\n",
    "ic_tileg = range(int(len(icg)/array_size)) ##[0,1,2,3]\n",
    "oc_tileg = range(int(len(ocg)/array_size)) ##[0,1,2,3]\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(njg)+padding*2).cuda()\n",
    "# a_pad.size() = [64, 32+2pad, 32+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))  ## mergin ni and nj index into nij\n",
    "# a_pad.size() = [64, (32+2pad)*(32+2pad)]\n",
    "a_tile = torch.zeros(len(ic_tileg), len(oc_tileg), array_size, a_pad.size(1)).cuda()\n",
    "for ic_tile in ic_tileg: #spatial\n",
    "    for oc_tile in oc_tileg: #spatial\n",
    "        a_tile[ic_tile,oc_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "        \n",
    "p_nijg = range(a_tile.size(3)) ## paded activation's nij group [0, ...34*34-1]\n",
    "\n",
    "######## Outputs ########\n",
    "o_nig = range(int((math.sqrt(len(nijg))+2*padding -(math.sqrt(len(kijg))- 1) - 1)/stride + 1)) #range(0, 32)\n",
    "o_njg = range(int((math.sqrt(len(nijg))+2*padding -(math.sqrt(len(kijg)) - 1) - 1)/stride + 1)) #range(0, 32)\n",
    "psum=torch.zeros(len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)).cuda() \n",
    "out = torch.zeros(len(ocg), len(o_nig), len(o_njg)).cuda()\n",
    "  \n",
    "\n",
    "######## Tiled 2D version ########\n",
    "\n",
    "for ic_tile in ic_tileg: #spatial\n",
    "    for oc_tile in oc_tileg: #spatial\n",
    "        for kij in kijg: #temporal\n",
    "            m = nn.Linear(array_size, array_size, bias=False)\n",
    "            m.weight = torch.nn.Parameter(w_int[oc_tile*array_size:(oc_tile+1)*array_size,ic_tile*array_size:(ic_tile+1)*array_size,kij])\n",
    "            for nij in p_nijg: #temporal\n",
    "                psum[ic_tile, oc_tile, :, nij, kij] = m(a_tile[ic_tile,oc_tile,:, nij]).cuda()\n",
    "\n",
    "### SFP accumulation ###\n",
    "for ni in o_nig:\n",
    "    for nj in o_njg:\n",
    "        for ki in kig:\n",
    "            for kj in kjg:\n",
    "                for ic_tile in ic_tileg:    \n",
    "                    for oc_tile in oc_tileg:                           \n",
    "                        out[oc_tile*array_size:(oc_tile+1)*array_size, ni, nj] = out[oc_tile*array_size:(oc_tile+1)*array_size, ni, nj] + \\\n",
    "                        psum[ic_tile, oc_tile, :, int(math.sqrt(len(p_nijg)))*(ni+ki) + (nj+kj), len(kig)*ki+kj]\n",
    "print(out.size())\n",
    "print(out.abs().sum())\n",
    "print(psum[0, 0, :, :, 0].size())\n",
    "print(len(ic_tileg))\n",
    "print(len(oc_tileg))\n",
    "print(a_tile.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f5be8cd-7f4c-4bbf-a458-edb17bd16e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 2, 2])\n",
      "torch.Size([16, 2, 2])\n",
      "8×8 tiled systolic difference = 0.0\n"
     ]
    }
   ],
   "source": [
    "ref = output_int[0]\n",
    "diff = (out - ref).abs().sum()\n",
    "print(ref.size())\n",
    "print(\"8×8 tiled systolic difference =\", diff.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf50bc22-530d-4131-ae9d-4e927e8826d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: systolic_export/weights/weight_tile0_kij0.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij1.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij2.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij3.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij4.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij5.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij6.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij7.txt\n",
      "Generated: systolic_export/weights/weight_tile0_kij8.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij0.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij1.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij2.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij3.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij4.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij5.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij6.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij7.txt\n",
      "Generated: systolic_export/weights/weight_tile1_kij8.txt\n",
      "Generated: systolic_export/activations/activation.txt\n",
      "Generated: systolic_export/psum/psum_kij0.txt\n",
      "Generated: systolic_export/psum/psum_kij1.txt\n",
      "Generated: systolic_export/psum/psum_kij2.txt\n",
      "Generated: systolic_export/psum/psum_kij3.txt\n",
      "Generated: systolic_export/psum/psum_kij4.txt\n",
      "Generated: systolic_export/psum/psum_kij5.txt\n",
      "Generated: systolic_export/psum/psum_kij6.txt\n",
      "Generated: systolic_export/psum/psum_kij7.txt\n",
      "Generated: systolic_export/psum/psum_kij8.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "def make_dir(d):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "def to_bin_signed(val, bits):\n",
    "    if val < 0:\n",
    "        val = (1 << bits) + val\n",
    "    return f'{val:0{bits}b}'\n",
    "\n",
    "\n",
    "\n",
    "def extract_layer27_int(model, images, act_bit=2, w_bit=4):\n",
    "\n",
    "    class Save:\n",
    "        def __init__(self):\n",
    "            self.out = None\n",
    "        def __call__(self, module, inp):\n",
    "            self.out = inp[0]   # only the input activation\n",
    "\n",
    "    hook = Save()\n",
    "    model.features[27].register_forward_pre_hook(hook)\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    images = images.to(device)\n",
    "    _ = model(images)  # run forward once\n",
    "    a_float = hook.out[0]   # [16,4,4]\n",
    "\n",
    "    layer = model.features[27]\n",
    "    act_alpha  = layer.act_alpha\n",
    "    act_scale  = act_alpha / (2**act_bit - 1)\n",
    "\n",
    "    w_alpha    = layer.weight_quant.wgt_alpha\n",
    "    w_scale    = w_alpha / (2**(w_bit-1) - 1)\n",
    "\n",
    "    act_q = layer.act_alq(a_float, act_alpha)\n",
    "    act_int = act_q / act_scale\n",
    "\n",
    "    weight_q = layer.weight_q    # already stored in fwd\n",
    "    weight_int = weight_q / w_scale\n",
    "\n",
    "    return act_int.detach(), weight_int.detach(), act_scale, w_scale\n",
    "\n",
    "\n",
    "# one file per each 4 TILES\n",
    "# def export_weight_tiles(weight_int, outdir=\"weights\", bits=4):\n",
    "\n",
    "#     make_dir(outdir)\n",
    "\n",
    "#     # reshape to [C_out, C_in, 9]\n",
    "#     w_flat = weight_int.reshape(16, 16, 9)\n",
    "\n",
    "#     array_size = 8\n",
    "#     ic_tiles = 2  # 16/8\n",
    "#     oc_tiles = 2\n",
    "\n",
    "#     tile_id = 0\n",
    "\n",
    "#     for oc_tile in range(oc_tiles):\n",
    "#         for ic_tile in range(ic_tiles):\n",
    "#             for kij in range(9):\n",
    "\n",
    "#                 W = w_flat[\n",
    "#                     oc_tile*8:(oc_tile+1)*8,\n",
    "#                     ic_tile*8:(ic_tile+1)*8,\n",
    "#                     kij\n",
    "#                 ]  # [8,8]\n",
    "\n",
    "#                 fname = f\"{outdir}/weight_tile{tile_id}_kij{kij}.txt\"\n",
    "#                 with open(fname, \"w\") as f:\n",
    "#                     f.write('#W[col0,row7], W[col1,row7], ..., W[col7,row7]#\\n')\n",
    "#                     f.write('#W[col0,row6], W[col1,row6], ..., W[col7,row6]#\\n')\n",
    "#                     f.write('#................#\\n')\n",
    "#                     for row in range(8):\n",
    "#                         for col in range(8):\n",
    "#                             val = int(round(W[col, 7 - row].item()))  # reversed row ordering\n",
    "#                             bits_bin = to_bin_signed(val, bits)\n",
    "#                             f.write(bits_bin)\n",
    "#                         f.write(\"\\n\")\n",
    "\n",
    "#                 print(\"Generated:\", fname)\n",
    "#             tile_id += 1\n",
    "\n",
    "import tensorflow as tf\n",
    "            \n",
    "# one file per output channel tile - 2 files in total\n",
    "def export_weight_tiles(weight_int, outdir=\"weights\", bits=4):\n",
    "\n",
    "    make_dir(outdir)\n",
    "\n",
    "    # reshape to [C_out, C_in, 9]\n",
    "    w_flat = weight_int.reshape(16, 16, 9)\n",
    "    \n",
    "    # print(\"THESE ARE NOT THE ACTUAL WEIGHTS!!!\")\n",
    "    # w_flat = torch.zeros((16, 16, 9), dtype=torch.int32)\n",
    "\n",
    "    # col_idx = torch.randint(0, 16, (1,)).item()  # pick random col\n",
    "    # w_flat[10, :, 8] = 1\n",
    "    \n",
    "    # # for i in range(16):\n",
    "    # for j in range(16):\n",
    "    #     w_flat[10,j,8] = j\n",
    "\n",
    "    array_size = 8\n",
    "    ic_tiles = 2  # 16/8\n",
    "    oc_tiles = 2\n",
    "\n",
    "    tile_id = 0\n",
    "\n",
    "\n",
    "# 0000 inch14(maps to 8th row) - 0010010101011110 - 0110(r:2,c:0) inch4 - 0011(r:1,c:0) inch2 - 1111(PE0,0)(w1) (PE00 gets inupt channels 0 & 1 not 0 and 8)\n",
    "# 0001 inch15 - 0000000110111111 - 0100(r:2,c:0) inch5 - 1111(r:1,c:0) inch3 - 1111(PE0,0)(w2)\n",
    "# 11001101000111010010 - 1100(r:2,c:1) - 0000(r:1,c:1) - 1100(r:0,c:1)\n",
    "# 01000011111011111110 - 1110(r:2,c:1) - 1100(r:1,c:1) - 0010(r:0,c:1)\n",
    "# 1111110101010011010100111110 - 0010(r:0,c:2)(w1)\n",
    "# 0010111011100011110111010011 - 1101(r:0,c:2)(w2)\n",
    "    \n",
    "    for oc_tile in range(oc_tiles):\n",
    "        for kij in range(9):\n",
    "\n",
    "            fname = f\"{outdir}/weight_tile{tile_id}_kij{kij}.txt\"\n",
    "            with open(fname, \"w\") as f:\n",
    "                # PE(row, col)-\n",
    "                # PE(2,1)-W1,PE(1,1)-W1\n",
    "                # PE(2,1)-W2,PE(1,1)-W2\n",
    "                # PE(\n",
    "                # f.write(f'octile={oc_tile},ic_tile=0 #W[col0,row7], W[col1,row7], ..., W[col7,row7]#\\n')\n",
    "                # f.write(f'octile={oc_tile},ic_tile=1 #W[col0,row7], W[col1,row7], ..., W[col7,row7]#\\n')\n",
    "                # f.write(f'octile={oc_tile},ic_tile=0 #W[col0,row6], W[col1,row6], ..., W[col7,row6]#\\n')\n",
    "                \n",
    "                # 2 rows together form 1 input channel\n",
    "                # inch:14 12 10 inch:8 ... inch:0\n",
    "                # inch:15 13 11 inch:9 ... inch:1\n",
    "                f.write(\"#......#\\n\")\n",
    "                f.write(\"#......#\\n\")\n",
    "                f.write(\"#......#\\n\")\n",
    "                for col in range(8):\n",
    "                # for ic_tile in range(ic_tiles): # 2\n",
    "                    W = w_flat[\n",
    "                            oc_tile*8:(oc_tile+1)*8,\n",
    "                            :,\n",
    "                            kij\n",
    "                        ]\n",
    "                    # print(W.size()) # [8,16]\n",
    "                    for row in range(1,16,2):\n",
    "                        val2 = int(round(W[col, 15 - row].item()))  # reversed row ordering\n",
    "                        bits_bin2 = to_bin_signed(val2, bits)\n",
    "                        f.write(bits_bin2)\n",
    "                    f.write(\"\\n\")\n",
    "                    for row in range(0,16,2):\n",
    "                        val = int(round(W[col, 15 - row].item()))  # reversed row ordering\n",
    "                        bits_bin = to_bin_signed(val, bits)\n",
    "                        f.write(bits_bin)\n",
    "                    f.write(\"\\n\")\n",
    "                    \n",
    "            print(\"Generated:\", fname)\n",
    "        tile_id += 1\n",
    "\n",
    "# def export_activation_tiles(act_int, outdir=\"activations\", bits=4):\n",
    "\n",
    "#     make_dir(outdir)\n",
    "\n",
    "#     array_size = 8\n",
    "#     ic_tiles = 2   # 16→2 tiles\n",
    "#     a_flat = act_int.reshape(16, 16)  # [C_in, H*W] = [16,16]\n",
    "\n",
    "#     for ic_tile in range(ic_tiles):\n",
    "#         tile_block = a_flat[ic_tile*8:(ic_tile+1)*8, :]   # [8,16]\n",
    "#         fname = f\"{outdir}/activation_ic{ic_tile}.txt\"\n",
    "#         with open(fname, \"w\") as f:\n",
    "#             f.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "#             f.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "#             f.write('#................#\\n')\n",
    "#             for t in range(16):           # time 0..15\n",
    "#                 for row in range(8):      # row 7..0\n",
    "#                     val = int(round(tile_block[7-row, t].item()))\n",
    "#                     bits_bin = to_bin_signed(val, bits)\n",
    "#                     f.write(bits_bin)\n",
    "#                 f.write(\"\\n\")\n",
    "#         print(\"Generated:\", fname)\n",
    "\n",
    "# def export_activation_tiles(act_int, outdir=\"activations\", bits=2):\n",
    "\n",
    "#     make_dir(outdir)\n",
    "\n",
    "#     array_size = 8\n",
    "#     ic_tiles = 2   # 16→2 tiles\n",
    "#     a_flat = act_int.reshape(16, 16)  # [C_in, H*W] = [16,16]\n",
    "#     nij_total = 16\n",
    "        \n",
    "#     fname = f\"{outdir}/activation.txt\"\n",
    "#     with open(fname, \"w\") as f:\n",
    "#         f.write('#w1time0row7[msb-lsb],w0time0row7[msb-lsb],w1time0row6[msb-lst],....,w0time0row0[msb-lst]#\\n')\n",
    "#         f.write('#w1time1row7[msb-lsb],w0time1row6[msb-lst],w0time1row6[msb-lst],....,w0time1row0[msb-lst]#\\n')\n",
    "#         f.write('#................#\\n')\n",
    "#         for t in range(nij_total):           # time 0..15\n",
    "#             for row in range(8):      # row 7..0\n",
    "#                 for ic_tile in range(ic_tiles):\n",
    "#                     # tile_block = a_flat[(1-ic_tile)*8:((1-ic_tile)+1)*8, :]   # [8,16]\n",
    "#                     tile_block = a_flat[(ic_tile)*8:((ic_tile)+1)*8, :]   # [8,16]\n",
    "#                     val = int(round(tile_block[7-row, t].item()))\n",
    "#                     bits_bin = to_bin_signed(val, bits)\n",
    "#                     # print(ic_tile, bits_bin)\n",
    "#                     f.write(bits_bin)\n",
    "#             f.write(\"\\n\")\n",
    "#     print(\"Generated:\", fname)\n",
    "\n",
    "from bitarray import bitarray\n",
    "\n",
    "# a first, then b (left to right)\n",
    "def interleave(A, B):\n",
    "    out = 0\n",
    "    out_pos = 62  # start at MSB of 64-bit output\n",
    "\n",
    "    for bitpos in range(30, -1, -2):  # 30, 28, ..., 0\n",
    "        b_chunk = (B >> bitpos) & 0b11\n",
    "        a_chunk = (A >> bitpos) & 0b11\n",
    "\n",
    "        # place B chunk (upper 2 bits)\n",
    "        out |= (b_chunk << out_pos)\n",
    "        out_pos -= 2\n",
    "\n",
    "        # place A chunk (lower 2 bits)\n",
    "        out |= (a_chunk << out_pos)\n",
    "        out_pos -= 2\n",
    "\n",
    "    return out\n",
    "\n",
    "def export_activation_tiles(act_int, outdir=\"activations\", bits=2):\n",
    "\n",
    "    make_dir(outdir)\n",
    "\n",
    "    array_size = 8\n",
    "    ic_tiles = 2   # 16→2 tiles\n",
    "    # a_flat = act_int.reshape(16, 16)  # [C_in, H*W] = [16,16]\n",
    "    nij_total = 16\n",
    "    # a_tile[ic_tile,oc_tile,:,:] = a_pad[ic_tile*array_size:(ic_tile+1)*array_size,:]\n",
    "    # ic tile, oc tile, array size, nij\n",
    "    fname = f\"{outdir}/activation.txt\"\n",
    "    # 16 rows = 16 nijs\n",
    "    # each row: 32 bit elts, 2 bit per element/input ch (16 in chs in total).\n",
    "    # \n",
    "    with open(fname, \"w\") as f:\n",
    "        # f.write('#time0row7[msb-lsb],time0row6[msb-lsb],time0row5[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "        # f.write('#time1row7[msb-lsb],time1row6[msb-lst],time1row5[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "        f.write('#................#\\n')\n",
    "        f.write('#................#\\n')\n",
    "        f.write('#................#\\n')\n",
    "        for t in range(0,nij_total,1): # time 0..15\n",
    "            for ic_tile in range(ic_tiles):\n",
    "                for row in range(8):      # row 7..0\n",
    "                    # tile_block = a_flat[(1-ic_tile)*8:((1-ic_tile)+1)*8, :]   # [8,16]\n",
    "                    # tile_block = a_flat[(ic_tile)*8:((ic_tile)+1)*8, :]   # [8,16]\n",
    "                    \n",
    "                    val0 = int(round(a_tile[ic_tile, 0, 7-row, t].item()))\n",
    "                    bits_bin0 = to_bin_signed(val0, bits)\n",
    "                    # val1 = int(round(a_tile[ic_tile, 0, 7-row, t+1].item()))\n",
    "                    # bits_bin1 = to_bin_signed(val1, bits)\n",
    "                    # bits_interleaved = interleave(val1, val0)\n",
    "                    # bits_interleaved_bin = to_bin_signed(bits_interleaved, bits*2)\n",
    "                    # print(bits_bin1)\n",
    "                    # print(bits_bin0)\n",
    "                    # print(bits_interleaved_bin)\n",
    "                    # print(\"\\n\")\n",
    "                    # f.write(bits_interleaved_bin)\n",
    "                    f.write(bits_bin0)\n",
    "            f.write(\"\\n\")\n",
    "    print(\"Generated:\", fname)\n",
    "\n",
    "\n",
    "\n",
    "## there should be 16 psum outputs - one per output channel\n",
    "# THIS FUNCITON IS WRONG: it writes the partial sums (during the convolution), not the final result\n",
    "def export_psum(psum, outdir=\"psum\", bits=16):\n",
    "\n",
    "    make_dir(outdir)\n",
    "\n",
    "    num_kij = psum.shape[4]\n",
    "    array_size = psum.shape[2]\n",
    "    nij_total = psum.shape[3]\n",
    "\n",
    "    oc_tile_id = 0\n",
    "    oc_tiles =2 \n",
    "    array_size = 8\n",
    "    C_out = array_size * oc_tiles\n",
    "    \n",
    "    # ic_tiles, oc_tiles, array_size, nij_total, num_kij = psum.shape\n",
    "\n",
    "\n",
    "    for kij in range(num_kij):\n",
    "        # itercnt = 0\n",
    "        fname = f\"{outdir}/psum_kij{kij}.txt\"\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(\"#every 16 bits in each column = psum at oc\\n\")\n",
    "            f.write(\"#every row is nij\\n\")\n",
    "            f.write(\"#time0 col7..col0 for kij slice\\n\")\n",
    "            \n",
    "            for t in range(nij_total): # time = line in file\n",
    "                \n",
    "                for oc_tile in range(oc_tiles): # out channel - column in file\n",
    "                    psum_x_0 = psum[0, oc_tile, :, :, kij]\n",
    "                    psum_x_1 = psum[1, oc_tile, :, :, kij]\n",
    "                    for oc in range(array_size):\n",
    "                        # itercnt+=1\n",
    "                        val = int(round(psum_x_0[7-oc, t].item())) # + int(round(psum_x_1[7-oc, t].item()))\n",
    "                        bits_bin = to_bin_signed(val, bits)\n",
    "                        # if(oc == 1):\n",
    "                            # print(bits_bin)\n",
    "                        f.write(bits_bin)\n",
    "                f.write(\"\\n\")\n",
    "        # print(itercnt)\n",
    "    \n",
    "        print(\"Generated:\", fname)\n",
    "\n",
    "# fixed version\n",
    "    # 256 BITS (16 output channels, 16 bits each) per row - each row is time 0, then time 1 ... each row is a specific time\n",
    "    # one file per kij -- per each kij we do nij many input macs and accumulations\n",
    "# we need the psum outputs of the sfu, the two weights' mac results (input channel) will be combined anyway \n",
    "# psum is the accumulatd result in the spec func unit -- 1 file per kij\n",
    "# d\n",
    "\n",
    "# def export_psum(psum, outdir=\"psum\", bits=16):\n",
    "    # make_dir(outdir)\n",
    "\n",
    "    # ic_tiles, oc_tiles, array_size, nij_total, num_kij = psum.shape\n",
    "\n",
    "    # C_out = oc_tiles * array_size\n",
    "    # row_in_tile = oc % array_size  # 0..7\n",
    "    # row_idx = array_size - 1 - row_in_tile\n",
    "\n",
    "\n",
    "    #     ## USES REVERSED ROW: - ie begins at row 7\n",
    "\n",
    "    #     for kij in range(num_kij):\n",
    "    #         fname = f\"{outdir}/psum_ch{oc}_kij{kij}.txt\"\n",
    "    #         with open(fname, \"w\") as f:\n",
    "    #             f.write(f\"# PSUM for output channel {oc}, kij={kij}\\n\")\n",
    "    #             f.write(\"# One value per time step (nij index)\\n\")\n",
    "    #             f.write(\"# One value per time step (nij index)\\n\")\n",
    "\n",
    "    #             for t in range(nij_total):\n",
    "    #                 # accumulate over all input-channel tiles\n",
    "    #                 acc_val = 0\n",
    "                    \n",
    "    #             for oc in range(C_out):\n",
    "    #                 oc_tile_id  = oc // array_size  # 0 or 1\n",
    "    #                 for ic_tile_id in range(ic_tiles):\n",
    "    #                     v = int(round(\n",
    "    #                         psum[ic_tile_id,   #  input tile (0..1)\n",
    "    #                              oc_tile_id,   #  output tile (0..1)\n",
    "    #                              row_idx,      # row in the 8x8 array\n",
    "    #                              t,            # time - nij\n",
    "    #                              kij].item()   # kernel index\n",
    "    #                     ))\n",
    "    #                     acc_val += v\n",
    "\n",
    "    #                 bits_bin = to_bin_signed(acc_val, bits)\n",
    "    #                 f.write(bits_bin + \"\\n\")\n",
    "\n",
    "    #         print(\"Generated:\", fname)\n",
    "\n",
    "# 128 bits per row - per outch: 16 bits - 8 columns in array\n",
    "# 4 rows (2x2)\n",
    "# 2 files (for each output tile)\n",
    "\n",
    "# bij: flattened index of the output matrix\n",
    "\n",
    "# file1:\n",
    "# bij0 : ch 7, ch 6, ... , ch 0\n",
    "# bij1 : ch 7, ch 6, ... , ch 0\n",
    "# bij2\n",
    "# bij3\n",
    "\n",
    "# file2:\n",
    "# bij0 : ch 15, ch 14, ... , ch 8\n",
    "# bij1 : ch 15, ch 14, ... , ch 8\n",
    "# bij2\n",
    "# bij3\n",
    "def export_final_output(out, outdir=\"final_psums\", bits=16, separate_files=True):\n",
    "\n",
    "    make_dir(outdir)\n",
    "\n",
    "    C_out, H_out, W_out = out.shape\n",
    "    # print(\"out.shape\", out.shape) # [16, 2, 2]\n",
    "    assert C_out == 16, f\"Expected 16 output channels, got {C_out}\"\n",
    "    \n",
    "    def write_value(f, val):\n",
    "        ival = int(round(val))\n",
    "        binval = to_bin_signed(ival, bits)\n",
    "        f.write(binval)\n",
    "        \n",
    "    bijg = 4 # flattened run var\n",
    "    tileg = 2 # oc tile count\n",
    "    array_size = 8\n",
    "    \n",
    "    out_flat = out.reshape(16, 4)\n",
    "\n",
    "    # testing:\n",
    "    # print(\"THESE ARE NOT THE ACTUAL OUTPUTS!!!\")\n",
    "    # out_flat = torch.zeros((16, 4), dtype=torch.int32)\n",
    "\n",
    "    # col_idx = torch.randint(0, 16, (1,)).item()  # pick random col\n",
    "    \n",
    "    # # for i in range(16):\n",
    "    # for j in range(4):\n",
    "    #     out_flat[8,j] = j\n",
    "    \n",
    "    for tile in range(tileg):\n",
    "        fname = f\"{outdir}/final_out_tile{tile}.txt\"\n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(\"#.....#\\n\")\n",
    "            f.write(\"#.....#\\n\")\n",
    "            f.write(\"#.....#\\n\")\n",
    "            for i in range(bijg): # rows - 4\n",
    "                for j in range((tile+1)*array_size - 1, (tile)*array_size-1, -1): # elts 0-7 or 8-15\n",
    "                    # print(f\"{j}\")\n",
    "                    write_value(f, out_flat[j, i].item())\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "    # if separate_files:\n",
    "    #     for oc in range(C_out):\n",
    "    #         fname = f\"{outdir}/final_out_ch{oc}.txt\"\n",
    "    #         with open(fname, \"w\") as f:\n",
    "    #             f.write(f\"#output: 16x2x2 - 16bits per out[ch][h][w] - [msb..lsb]\\n\")\n",
    "    #             f.write(f\"#line0: out[ch={oc}][0][0], out[ch={oc}][0][1]\\n\")\n",
    "    #             f.write(f\"#line1: out[ch={oc}][1][0], out[ch={oc}][1][1]\\n\")\n",
    "    #             for i in range(H_out):\n",
    "    #                 for j in range(W_out):\n",
    "    #                     write_value(f, out[oc, i, j].item())\n",
    "    #                 f.write(\"\\n\")\n",
    "    #         print(\"Generated:\", fname)\n",
    "\n",
    "def export_systolic_layer27(model, images, psum, outdir=\"systolic_export\"):\n",
    "\n",
    "    make_dir(outdir)\n",
    "\n",
    "    act_int, weight_int, act_scale, w_scale = extract_layer27_int(model, images)\n",
    "\n",
    "    export_weight_tiles(weight_int, outdir=f\"{outdir}/weights\")\n",
    "\n",
    "    export_activation_tiles(act_int, outdir=f\"{outdir}/activations\")\n",
    "\n",
    "    export_psum(psum, outdir=f\"{outdir}/psum\")\n",
    "\n",
    "    export_final_output(out, outdir=f\"{outdir}/outputs\")\n",
    "    \n",
    "    torch.save(act_int,  f\"{outdir}/act_int.pt\")\n",
    "    torch.save(weight_int, f\"{outdir}/weight_int.pt\")\n",
    "    torch.save(psum, f\"{outdir}/psum_raw.pt\")\n",
    "\n",
    "export_systolic_layer27(model, images, psum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
